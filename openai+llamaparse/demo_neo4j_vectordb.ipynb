{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF parsing using LlamaParse for knowledge graph creation in Neo4j \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This Python notebook offers a comprehensive guide on leveraging LlamaParse to extract information from PDF documents and subsequently store this extracted content into a Neo4j graph database. Designed with practicality in mind, this tutorial caters to developers, data scientists, and tech enthusiasts interested in document processing, information extraction, and graph database technologies.\n",
    "\n",
    "Key Features of the Notebook:\n",
    "\n",
    "<b>1. Setting Up the Environment:</b> Step-by-step instructions on setting up your Python environment, including the installation of necessary libraries and tools such as LlamaParse and the Neo4j database driver.\n",
    "\n",
    "<b>2. PDF Document Processing</b>: Demonstrates how to use LlamaParse to read PDF documents, extract relevant information (such as text, tables, and images), and transform this information into a structured format suitable for database insertion.\n",
    "\n",
    "<b>3. The Graph Model for docoment</b>: Guidance on designing an effective graph model that represents the relationships and entities extracted from your PDF documents, ensuring optimal structure for querying and analysis. \n",
    "\n",
    "<b>4. Storing Extracted Data in Neo4j</b>: Detailed code examples showing how to connect to a Neo4j database from Python, create nodes and relationships based on the extracted data, and execute Cypher queries to populate the database.\n",
    "\n",
    "<b>5. Generating and Storing Text Embeddings</b>: Using program created in the past to generate text embedding via OpenAI API call and store embedding as vector in Neo4j. \n",
    "\n",
    "<b>6. Querying and Analyzing Data</b>: Examples of Cypher queries to retrieve and analyze the stored data, illustrating how Neo4j can uncover insights and relationships hidden within your PDF content.\n",
    "\n",
    "<b>7. Conclusions</b>: Tips on best practices for processing PDFs, designing graph schemas, and optimizing Neo4j queries, along with common troubleshooting advice for potential issues encountered during the process.\n",
    "\n",
    "For a more comprehensive introduction of LlamaParse, please check <a href=\"https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b\">this artile</a>.\n",
    "\n",
    "Note for this example, it is required for `llama_index >=0.10.4` version. If `pip install --upgrade <package_name>` didn't work, you may use `pip uninstall <package_name>` and install required package again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Setting up the environment</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install llama-index\n",
    "!pip3 install llama-index-core\n",
    "!pip3 install llama-index-embeddings-openai\n",
    "!pip3 install llama-parse\n",
    "!pip3 install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-31 09:56:20--  https://raw.githubusercontent.com/Joshua-Yu/graph-rag/main/openai%2Bllamaparse/InjuredWorkerGuidebookCalifornia.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3779314 (3.6M) [application/octet-stream]\n",
      "Saving to: ‘./insurance.pdf’\n",
      "\n",
      "./insurance.pdf     100%[===================>]   3.60M  8.94MB/s    in 0.4s    \n",
      "\n",
      "2024-03-31 09:56:21 (8.94 MB/s) - ‘./insurance.pdf’ saved [3779314/3779314]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/Joshua-Yu/graph-rag/main/openai%2Bllamaparse/InjuredWorkerGuidebookCalifornia.pdf' -O './insurance.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. PDF document processing</h3>\n",
    "\n",
    "We need OpenAI and LlamaParse API keys to run the project. \n",
    "\n",
    "For more information of how to get OpenAI API key, please visit <a href=\"https://openai.com/\">here</a>.\n",
    "\n",
    "For more information of how to get LlamaParse key, please visit <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/llama_parse/\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "# API access to llama-cloud\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-iIqGQRKcuCaKQpW9clrlzhrQZcXvo6o2IWannIh6cqP4xhce\"\n",
    "\n",
    "# Using OpenAI API for embeddings/llms\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import  VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "EMBEDDING_MODEL  = \"text-embedding-3-small\"\n",
    "GENERATION_MODEL = \"gpt-4\"\n",
    "\n",
    "llm = OpenAI(model=GENERATION_MODEL)\n",
    "\n",
    "Settings.llm = llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using brand new `LlamaParse` PDF reader for PDF Parsing</h4>\n",
    "\n",
    "we also compare two different retrieval/query engine strategies:\n",
    "1. Using raw Markdown text as nodes for building index and apply simple query engine for generating the results;\n",
    "2. Using `MarkdownElementNodeParser` for parsing the `LlamaParse` output Markdown results and building recursive retriever query engine for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id b144a386-f267-45c3-ab36-db70ef57b4ac\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "pdf_file_name = './insurance.pdf'\n",
    "\n",
    "documents = LlamaParse(result_type=\"markdown\").load_data(pdf_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check loaded documents\n",
    "\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.doc_id)\n",
    "    print(doc.text[:500] + '...')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the documents using MarkdownElementNodeParser\n",
    "\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "\n",
    "node_parser = MarkdownElementNodeParser(llm=llm, num_workers=8)\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nodes into objects\n",
    "\n",
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Check parsed node objects \n",
    "\n",
    "print(f\"Number of nodes: {len(base_nodes)}\")\n",
    "\n",
    "# print(base_nodes[0].dict())\n",
    "\n",
    "for node in base_nodes[:5]: \n",
    "    print(f\"id:{node.node_id}\")\n",
    "    print(f\"hash:{node.hash}\")\n",
    "    print(f\"parent:{node.parent_node}\")\n",
    "    print(f\"prev:{node.prev_node}\")\n",
    "    print(f\"next:{node.next_node}\")\n",
    "    print(f\"ref_doc:{node.ref_doc_id}\")\n",
    "    print(f\"type:{node.get_type()}\")\n",
    "    print(f\"class:{node.class_name()}\")\n",
    "    print(f\"content:{node.get_content()[:20]}\")\n",
    "    print(f\"metadata:{node.metadata}\")\n",
    "    print(f\"extra:{node.extra_info}\")\n",
    "\n",
    "    node_json = json.loads(node.json())\n",
    "\n",
    "    print(f\"start_idx:{node_json['start_char_idx']}\")\n",
    "    print(f\"end_idx:{node_json['end_char_idx']}\")\n",
    "\n",
    "    print(\"=====================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TABLE_REF_SUFFIX = '_table_ref'\n",
    "TABLE_ID_SUFFIX  = '_table'\n",
    "\n",
    "# Check parsed objects \n",
    "\n",
    "print(f\"Number of objects: {len(objects)}\")\n",
    "\n",
    "for node in objects: \n",
    "    print(f\"id:{node.node_id}\")\n",
    "    print(f\"hash:{node.hash}\")\n",
    "    print(f\"parent:{node.parent_node}\")\n",
    "    print(f\"prev:{node.prev_node}\")\n",
    "    print(f\"next:{node.next_node}\")\n",
    "\n",
    "    # Object is a Table\n",
    "    if node.node_id[-1 * len(TABLE_REF_SUFFIX):] == TABLE_REF_SUFFIX:\n",
    "\n",
    "        if node.next_node is not None:\n",
    "            next_node = node.next_node\n",
    "        \n",
    "            print(f\"next_node metadata:{next_node.metadata}\")\n",
    "            print(f\"next_next_node:{next_next_nod_id}\")\n",
    "\n",
    "            obj_metadata = json.loads(str(next_node.json()))\n",
    "\n",
    "            print(str(obj_metadata))\n",
    "\n",
    "            print(f\"def:{obj_metadata['metadata']['table_df']}\")\n",
    "            print(f\"summary:{obj_metadata['metadata']['table_summary']}\")\n",
    "\n",
    "\n",
    "    print(f\"next:{node.next_node}\")\n",
    "    print(f\"type:{node.get_type()}\")\n",
    "    print(f\"class:{node.class_name()}\")\n",
    "    print(f\"content:{node.get_content()[:200]}\")\n",
    "    print(f\"metadata:{node.metadata}\")\n",
    "    print(f\"extra:{node.extra_info}\")\n",
    "    \n",
    "    node_json = json.loads(node.json())\n",
    "\n",
    "    print(f\"start_idx:{node_json.get('start_char_idx')}\")\n",
    "    print(f\"end_idx:{node_json['end_char_idx']}\")\n",
    "\n",
    "    if 'table_summary' in node_json: \n",
    "        print(f\"summary:{node_json['table_summary']}\")\n",
    "\n",
    "    print(\"=====================================\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Graph model for parsed document</h3>\n",
    "\n",
    "Regardless which PDF parsing tool to use, to save results into Neo4j as a knowledge graph, the graph schema is in fact quite consistent. The links to relevant posts are here: \n",
    "  - Building A Graph+LLM Powered RAG Application from PDF Documents (using LLMSherpa) (<a href=\"https://medium.com/neo4j/building-a-graph-llm-powered-rag-application-from-pdf-documents-24225a5baf01\">link</a>)\n",
    "  - Integrating unstructured.io with Neo4j AuraDB to Build Document Knowledge Graph(<a href=\"https://medium.com/@yu-joshua/integrating-unstructured-io-with-neo4j-auradb-to-build-document-knowledge-graph-8b375d22ee2b\">link</a>)\n",
    "\n",
    "\n",
    "<img alt=\"document graph schema\" width=\"800\" src=\"https://raw.githubusercontent.com/Joshua-Yu/graph-rag/main/openai%2Bllamaparse/document_graph_schema.png\"></img>  \n",
    "\n",
    "In this project, the samiliar graph model will be used. Let's start with graph database schema definition: \n",
    "  1) Uniqueness constraint on key property\n",
    "  2) Vector index on embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "NEO4J_URL = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"Neo4j123\"\n",
    "NEO4J_DATABASE = \"insurance\"\n",
    "\n",
    "def initialiseNeo4jSchema():\n",
    "    cypher_schema = [\n",
    "        \"CREATE CONSTRAINT sectionKey IF NOT EXISTS FOR (c:Section) REQUIRE (c.key) IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT chunkKey IF NOT EXISTS FOR (c:Chunk) REQUIRE (c.key) IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT documentKey IF NOT EXISTS FOR (c:Document) REQUIRE (c.url_hash) IS UNIQUE;\",\n",
    "        \"CREATE VECTOR INDEX `chunkVectorIndex` IF NOT EXISTS FOR (e:Embedding) ON (e.value) OPTIONS { indexConfig: {`vector.dimensions`: 1536, `vector.similarity_function`: 'cosine'}};\"\n",
    "    ]\n",
    "\n",
    "    driver = GraphDatabase.driver(NEO4J_URL, database=NEO4J_DATABASE, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for cypher in cypher_schema:\n",
    "            session.run(cypher)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create constraints and indexes\n",
    "\n",
    "initialiseNeo4jSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Storing Extracted Data in Neo4j</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start saving documents to Neo4j...\n",
      "1 documents saved.\n",
      "Start saving nodes to Neo4j...\n",
      "49 nodes saved.\n",
      "Start saving objects to Neo4j...\n",
      "Start creating chunks for each TEXT Section...\n",
      "25 objects saved.\n",
      "=================DONE====================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URL, database=NEO4J_DATABASE, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# ================================================\n",
    "# 1) Save documents\n",
    "\n",
    "print(\"Start saving documents to Neo4j...\")\n",
    "i = 0\n",
    "with driver.session() as session:\n",
    "    for doc in documents:\n",
    "        cypher = \"MERGE (d:Document {url_hash: $doc_id}) ON CREATE SET d.url=$url;\"\n",
    "        session.run(cypher, doc_id=doc.doc_id, url=doc.doc_id)\n",
    "        i = i + 1\n",
    "    session.close()\n",
    "\n",
    "print(f\"{i} documents saved.\")\n",
    "\n",
    "# ================================================\n",
    "# 2) Save nodes\n",
    "\n",
    "print(\"Start saving nodes to Neo4j...\")\n",
    "\n",
    "i = 0\n",
    "with driver.session() as session:\n",
    "    for node in base_nodes: \n",
    "\n",
    "        # >>1 Create Section node\n",
    "        cypher  = \"MERGE (c:Section {key: $node_id})\\n\"\n",
    "        cypher += \" FOREACH (ignoreMe IN CASE WHEN c.type IS NULL THEN [1] ELSE [] END |\\n\"\n",
    "        cypher += \"     SET c.hash = $hash, c.text=$content, c.type=$type, c.class=$class_name, c.start_idx=$start_idx, c.end_idx=$end_idx )\\n\"\n",
    "        cypher += \" WITH c\\n\"\n",
    "        cypher += \" MATCH (d:Document {url_hash: $doc_id})\\n\"\n",
    "        cypher += \" MERGE (d)<-[:HAS_DOCUMENT]-(c);\"\n",
    "\n",
    "        node_json = json.loads(node.json())\n",
    "\n",
    "        session.run(cypher, node_id=node.node_id, hash=node.hash, content=node.get_content(), type='TEXT', class_name=node.class_name()\n",
    "                          , start_idx=node_json['start_char_idx'], end_idx=node_json['end_char_idx'], doc_id=node.ref_doc_id)\n",
    "\n",
    "        # >>2 Link node using NEXT relationship\n",
    "\n",
    "        if node.next_node is not None: # and node.next_node.node_id[-1*len(TABLE_REF_SUFFIX):] != TABLE_REF_SUFFIX:\n",
    "            cypher  = \"MATCH (c:Section {key: $node_id})\\n\"    # current node should exist\n",
    "            cypher += \"MERGE (p:Section {key: $next_id})\\n\"    # previous node may not exist\n",
    "            cypher += \"MERGE (p)<-[:NEXT]-(c);\"\n",
    "\n",
    "            session.run(cypher, node_id=node.node_id, next_id=node.next_node.node_id)\n",
    "\n",
    "        if node.prev_node is not None:  # Because tables are in objects list, so we need to link from the opposite direction\n",
    "            cypher  = \"MATCH (c:Section {key: $node_id})\\n\"    # current node should exist\n",
    "            cypher += \"MERGE (p:Section {key: $prev_id})\\n\"    # previous node may not exist\n",
    "            cypher += \"MERGE (p)-[:NEXT]->(c);\"\n",
    "\n",
    "            if node.prev_node.node_id[-1 * len(TABLE_ID_SUFFIX):] == TABLE_ID_SUFFIX:\n",
    "                prev_id = node.prev_node.node_id + '_ref'\n",
    "            else:\n",
    "                prev_id = node.prev_node.node_id\n",
    "\n",
    "            session.run(cypher, node_id=node.node_id, prev_id=prev_id)\n",
    "\n",
    "        i = i + 1\n",
    "    session.close()\n",
    "\n",
    "print(f\"{i} nodes saved.\")\n",
    "\n",
    "# ================================================\n",
    "# 3) Save objects\n",
    "\n",
    "print(\"Start saving objects to Neo4j...\")\n",
    "\n",
    "i = 0\n",
    "with driver.session() as session:\n",
    "    for node in objects:               \n",
    "        node_json = json.loads(node.json())\n",
    "\n",
    "        # Object is a Table, then the ????_ref_table object is created as a Section, and the table object is Chunk\n",
    "        if node.node_id[-1 * len(TABLE_REF_SUFFIX):] == TABLE_REF_SUFFIX:\n",
    "            if node.next_node is not None:  # here is where actual table object is loaded\n",
    "                next_node = node.next_node\n",
    "\n",
    "                obj_metadata = json.loads(str(next_node.json()))\n",
    "\n",
    "                cypher  = \"MERGE (s:Section {key: $node_id})\\n\"\n",
    "                cypher += \"WITH s MERGE (c:Chunk {key: $table_id})\\n\"\n",
    "                cypher += \" FOREACH (ignoreMe IN CASE WHEN c.type IS NULL THEN [1] ELSE [] END |\\n\"\n",
    "                cypher += \"     SET c.hash = $hash, c.definition=$content, c.text=$table_summary, c.type=$type, c.start_idx=$start_idx, c.end_idx=$end_idx )\\n\"\n",
    "                cypher += \" WITH s, c\\n\"\n",
    "                cypher += \" MERGE (s) <-[:UNDER_SECTION]- (c)\\n\"\n",
    "                cypher += \" WITH s MATCH (d:Document {url_hash: $doc_id})\\n\"\n",
    "                cypher += \" MERGE (d)<-[:HAS_DOCUMENT]-(s);\"\n",
    "\n",
    "                session.run(cypher, node_id=node.node_id, hash=next_node.hash, content=obj_metadata['metadata']['table_df'], type='TABLE'\n",
    "                                  , start_idx=node_json['start_char_idx'], end_idx=node_json['end_char_idx']\n",
    "                                  , doc_id=node.ref_doc_id, table_summary=obj_metadata['metadata']['table_summary'], table_id=next_node.node_id)\n",
    "                \n",
    "            if node.prev_node is not None:\n",
    "                cypher  = \"MATCH (c:Section {key: $node_id})\\n\"    # current node should exist\n",
    "                cypher += \"MERGE (p:Section {key: $prev_id})\\n\"    # previous node may not exist\n",
    "                cypher += \"MERGE (p)-[:NEXT]->(c);\"\n",
    "\n",
    "                if node.prev_node.node_id[-1 * len(TABLE_ID_SUFFIX):] == TABLE_ID_SUFFIX:\n",
    "                    prev_id = node.prev_node.node_id + '_ref'\n",
    "                else:\n",
    "                    prev_id = node.prev_node.node_id\n",
    "                \n",
    "                session.run(cypher, node_id=node.node_id, prev_id=prev_id)\n",
    "                \n",
    "        i = i + 1\n",
    "    session.close()\n",
    "\n",
    "# ================================================\n",
    "# 4) Create Chunks for each Section object of type TEXT\n",
    "# If there are changes to the content of TEXT section, the Section node needs to be recreated\n",
    "\n",
    "print(\"Start creating chunks for each TEXT Section...\")\n",
    "\n",
    "with driver.session() as session:\n",
    "\n",
    "    cypher  = \"MATCH (s:Section) WHERE s.type='TEXT' \\n\"\n",
    "    cypher += \"WITH s CALL {\\n\"\n",
    "    cypher += \"WITH s WITH s, split(s.text, '\\n') AS para\\n\"\n",
    "    cypher += \"WITH s, para, range(0, size(para)-1) AS iterator\\n\"\n",
    "    cypher += \"UNWIND iterator AS i WITH s, trim(para[i]) AS chunk, i WHERE size(chunk) > 0\\n\"\n",
    "    cypher += \"CREATE (c:Chunk {key: s.key + '_' + i}) SET c.type='TEXT', c.text = chunk, c.seq = i \\n\"\n",
    "    cypher += \"CREATE (s) <-[:UNDER_SECTION]-(c) } IN TRANSACTIONS OF 500 ROWS ;\"\n",
    "    \n",
    "    session.run(cypher)\n",
    "    \n",
    "    session.close()\n",
    "\n",
    "\n",
    "print(f\"{i} objects saved.\")\n",
    "\n",
    "print(\"=================DONE====================\")\n",
    "\n",
    "driver.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Generating and Storing Text Embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def get_embedding(client, text, model):\n",
    "    response = client.embeddings.create(\n",
    "                    input=text,\n",
    "                    model=model,\n",
    "                )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def LoadEmbedding(label, property):\n",
    "    driver = GraphDatabase.driver(NEO4J_URL, auth=(NEO4J_USER, NEO4J_PASSWORD), database=NEO4J_DATABASE)\n",
    "    openai_client = OpenAI (api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # get chunks in document, together with their section titles\n",
    "        result = session.run(f\"MATCH (ch:{label}) RETURN id(ch) AS id, ch.{property} AS text\")\n",
    "        # call OpenAI embedding API to generate embeddings for each proporty of node\n",
    "        # for each node, update the embedding property\n",
    "        count = 0\n",
    "        for record in result:\n",
    "            id = record[\"id\"]\n",
    "            text = record[\"text\"]\n",
    "            \n",
    "            # For better performance, text can be batched\n",
    "            embedding = get_embedding(openai_client, text, EMBEDDING_MODEL)\n",
    "            \n",
    "            # key property of Embedding node differentiates different embeddings\n",
    "            cypher = \"CREATE (e:Embedding) SET e.key=$key, e.value=$embedding, e.model=$model\"\n",
    "            cypher = cypher + \" WITH e MATCH (n) WHERE id(n) = $id CREATE (n) -[:HAS_EMBEDDING]-> (e)\"\n",
    "            session.run(cypher,key=property, embedding=embedding, id=id, model=EMBEDDING_MODEL) \n",
    "            count = count + 1\n",
    "\n",
    "        session.close()\n",
    "        \n",
    "        print(\"Processed \" + str(count) + \" \" + label + \" nodes for property @\" + property + \".\")\n",
    "        return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1216 Chunk nodes for property @text.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For smaller amount (<2000) of text data to embed\n",
    "LoadEmbedding(\"Chunk\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Querying Document Knowledge Graph</h3>\n",
    "\n",
    "\n",
    "Let's open Neo4j Browser to check the loaded document graph. \n",
    "\n",
    "Type `MATCH (n:Section) RETURN n` in text box and we will see a chain of sections of the document. By clicking and expanding a Section node, we can see its connected Chunk nodes. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Joshua-Yu/graph-rag/main/openai%2Bllamaparse/query_document_visualization.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "If a Section node has type `TEXT`, it has a group of Chunk nodes and each stores a paragraph in the <i>text</i> property. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Joshua-Yu/graph-rag/main/openai%2Bllamaparse/text_node_property.png\" width=\"240\" />\n",
    "\n",
    "If a Section node has type `TABLE`, it only has one Chunk node, with <i>text</i> property storing the summary of table contents, and <i>definition</i> property storing contents of the table. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Joshua-Yu/graph-rag/main/openai%2Bllamaparse/table_node_property.png\" width=\"240\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Every Chunk node is connected with an Embedding node which stores the embedding of the text content of the Chunk node. In the beginning of this project, a vector index has been defined to allow us to perform similarity search more efficiently.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
